# Abstract
How to locate an target, from receiving a sound at the beginning, and then accociate the sound with the image, and finally find it?  

In paper, the author proposed a [transformer-based model](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) learning the association between how objects look and how they sound to solve this problem. A goal descriptor captures both spatial and semantic properties of the target, and the persistent multimodal memory to reach the goal even when the sound (acoustic event) stops.


# Spotlight
1. Observation encoder: each time step t, the \O_t 

# What to learn
