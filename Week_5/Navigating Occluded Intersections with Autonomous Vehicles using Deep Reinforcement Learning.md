# Citation

# Abstract


# Spotlight
* Cons of imitation learning: if the agent finds itself in a state that is not part of the training data.
* UGV has a wide variety of state and action representations, and the selection of the representation had a significant impact on the agentâ€™s ability to learn

A very conclusive way to explain DQN:
In Deep Q-learning [15], the optimal value function is approximated with a neural network Q^\*(s; a) Q(s; a; \theta) with parameters \theta. The action value function is learned by iteratively minimizing the error between the expected return and the state-action value predicted by the network

![equ_1](https://github.com/hynpu/ece7970_winter2021/blob/main/Week_5/Figures/equ.JPG)

# What to learn
Monte Carlo Planning
