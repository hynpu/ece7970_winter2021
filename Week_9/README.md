# stochastic gradient descent (SGD)

# why the replay buffer and sampling is not applicable in policy gradient methods?

For policy gradient methods, as most of them are on-policy, which means that we have to train on samples generated by our current policy, so remembering old transitions will not be possible anymore. If you tried to use the replay buffer for a policy gradient method, the resulting policy gradient will be for the old policy used to generate the samples and not for the current policy that you want to update.

# overhead 

In computer science, overhead is any combination of excess or indirect computation time, memory, bandwidth, or other resources that are required to perform a specific task. It is a special case of engineering overhead. Overhead can be a deciding factor in software design, with regard to structure, error correction, and feature inclusion. Examples of computing overhead may be found in functional programming[citation needed], data transfer, and data structures.

# unpack_batch in ptan

# mutable-and-immutable

# with in python

# pop_total_rewards what is this?

# multiprocessing get() method

# why net.zero_grad()